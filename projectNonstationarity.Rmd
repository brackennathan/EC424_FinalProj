---
title: "Project_Nonstationarity"
author: "Micah Klein and Nathan Bracken"
date: "03/09/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(here, data.table, tseries, tidymodels, janitor, stringr, tframe, skimr, data.table)
```

```{r, data-load}
food_df = here("Data", "daily-per-capita-caloric-supply.csv") |> fread()
```

# Question at Hand 
  # "How do we locate a Unit Root?"
    #"Yes, with a test it is visual, but can it be automated/predicted?"


```{r, data-prep}
## creatign a function for cross fold validation that takes some time period i and makes folds of data up to that period

skim(food_df)

food_df = as.data.frame(apply(food_df,2,function(x)gsub('\\s+', '',x)))

food_df = food_df %>% split(unique(food_df$Entity))

food_c = food_df %>% filter(Year >= 1961 & Code != "") %>%
  mutate(daily_c = `Daily caloric supply (OWID based on UN FAO & historical sources)` ) %>%
  select(-c(`Daily caloric supply (OWID based on UN FAO & historical sources)`, Code))

food_c = ts(food_c)

food_c = as.data.frame(food_c)
#smallest that makes sense is k = 3
series_1 = food_c %>% filter(Entity == "1")

test_i = adf.test(series_1$daily_c, k = 3)

test_i$p.value



## size of the interval for project /identification of nonstationarity not the same as adf.test = k


food_df = food_df %>% 
  mutate(i_nonst = lapply(
    X = 
  ))
## creating a recipe 
cal_rec = food_train %>% recipe(~.) %>% 
  step_rename("pcap_dcal" = "Daily caloric supply (OWID based on UN FAO & historical sources)") %>% 
  step_
```


# Vizualization 
```{r}
ts.plot(food_ts)
```

t = length of series
s = start of series
b = beg of interval
e = end of interval

write a variable for interval length
  - try iterate along the start of the interval
  - start at the same time in each series
  
# working backwards is much more interesting than forwards
# accessing a subsection of a table

# we can try different sizes of intervals
# different sizes of windows
# tune our hyperparameters


# Defining a function that takes the intervals of a series
```{r}
interval = seq(from = 1, to = 50)

# we have series of a country 1 to 58 with calorie count
   # increasing size interval

# function that accepts some window and returns the dickey fuller test over the window
e_intv = function(series, i_size) {
# find the length of the vector
  vec = lapply(
    x = series, 
    # make sure that start - end \geq 3
    start = 1:nrow(),
    end = end
  )
  # test determines if window is non stationary
  test_i = adf.test(interval_i, k = 3)
  return(test_i$p.value)
}

## null hypothesis 
  ## In statistics and econometrics, an augmented Dickey–Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. It is an augmented version of the Dickey–Fuller test for a larger and more complicated set of time series models.
  ## P-value high - > high probability of null -> non station
  ## P-value low -> low probability null -> reject null -> station
        # consider de trending and see how p-values change
start_f = function(series, end, size){
  filtered_i = filter(series, Year <= end & Year > end - size) 
  tested_i = adf.test(filtered_i$daily_c, k = 1)
  if(is.na(tested_i$p.value) == TRUE){
    tested_i$p.value = 1
  }
  return(tested_i$p.value)
}

start_f(series_1, 58, 4)

# increasing the size of the interval makes it non stationary
unique(food_c$Entity)


```


```{r, warning = false, message = false}
e_frame = data.frame()
# set warnings==false
for (i in 1:1){
  series_i = filter(food_c, Entity == i)
  for (o in 4:8){
    vec_o = c(rep(1,o))
    for (p in o:57){
      p_val = start_f(series_i, p, o)
      vec_o = c(vec_o, p_val)
    }
    series_i = cbind(series_i, vec_o)
  }
  e_frame = rbind.data.frame(e_frame, series_i)
}
```
over the different countries(
filter for series_i 
for loop (o) over diffrent sizes( 
for loop (p) over diffrent end years(
add (start_f (series_i, p, o)) to vector 
)

```{r}



a = start_f(series = series_1, end = 4, i_size = 4)

end_v
size_v

x = rnorm(1000)
adf.test(x)

lapply()


c# taking the series and producing a metadata containing the results of each interval  
df_i = function(series, start, end){
  window = e_intv(series, start, end)
tibble(
    interation_n = i,
    w_start = window_i$start,
    w_end = window_i$end,
    nonstat_p = window
  )
}

#write another function here grabbing each series
e_intv(series = series_1, start = 1 , end = 58)

e_intv()
# function that accepts some vector and returns a tibble logging the window start and end


tfwindow(foood_ts)

pivot_wider(foood_ts)

foood_ts[]

## Setting base iterations
nobs = 1000
nsim = 1000
NB = rep(0,nsim)
DF = rep(0,nsim)

## Simulating a process
wiener = function(nobs) {
e = rnorm(nobs)
y = cumsum(e)
ym1 = y[1:(nobs-1)]
intW2 = nobs^(-2) * sum(ym1^2)
intWdW = nobs^(-1) * sum(ym1*e[2:nobs])
ans = list(intW2=intW2,
intWdW=intWdW)
ans
}

# iterating over the generatied 
for (i in 1:nobs) {
BN.moments = wiener(nobs)
NB[i] = BN.moments$intWdW/BN.moments$intW2
DF[i] = BN.moments$intWdW/sqrt(BN.moments$intW2)
}
```

# Linear Model
```{r}

```

# Logistic Lasso
```{r}

```

Data Gather data.

data -> would like faily consistent data and fairly inconsistent data
        -> comparison to other sreies -> known martingale defined? 
        
outcomes -> dickey fuller unit root 
             - augmented dickey fuller unit root
              
predictors ->   residual size 
                level of previous period
                amount of change in levels of each period - these are likely correlated
                amount of change in residuals - correlated

considering classification of underlying processes 
  - decision trees
  - logistic multifactor regression
  
Executive summary
The executive summary should be organized and written well. Specifically, it should follow this outline.

Paragraph 2: The big picture

your project's question

Predicting whether or not a segment of time series information will be non stationary or not.

why the question is important/interesting

The underlying stochastic processes of information that we see are composed of random variables. Approximating the future of a given random process requires the identification of the components of the process and the underlying distribution of the given random variable. 

Understanding that a process or time series will have non stationary means that we will be able to identify if the underlying random variable for a random process will be different than the existing one.

what makes this a prediction problem

This is a prediction problem because we are concerned with evaluating the probability that some stream of information will become unstable in a period that we have not yet experienced. While we may know if a certain process is made up of moving averages and auto regressive components 

Paragraph 3: Data

sources
cleaning
challenges
- managing all of the countries
shortcomings
- general lack of predictors


Paragraph 4: Methods

learning methods applied
tuned parameters
method for tuning
Paragraph 5: Results and conclusion

how you measure performance/success
how your models perform
what you think limited your performance
what you learned in this process
Paragraph 1: Brief overview of paragraphs 2–5


Test Randomly select approximately 20% of your data for a test set. (Don't train on it until everything is done.)

Train Apply "best" techniques to clean, train, and predict. Use four different algorithms—one of which should be a linear-regression-based model (unless it is not possible in your context). Examples of different algorithms: Logistic regression, lasso, random forests, SVM.

CV error Estimate your error (with an appropriately chosen metric) using cross validation.

Test Test your performance with the held out data.

Evaluation of your group member's contribution (submitted individually)

Project workflow

1. Generate/find data
2. Test series with 

# fancy things that would be nice in the simplest package
## relative file path
## environment json
## make file/docker file

# Adjust for pulling data for nonstationarity
```{r}
## DBI, bigrquery,dbplyr

billing_id = Sys.getenv("GCE_TIDYTEXT_PROJECT_ID")

books_bq_con = dbConnect(bigrquery::bigquery(),
                project ="gdelt-bq", 
                dataset = "internetarchivebooks",
                billing = billing_id,
                use_legacy_sql = FALSE)

books_table_1920 = tbl(books_bq_con,'1920')
```


# Creating Bags of "Non stationary increments"
```{r}
# define a bag
# randomly select number of non stationary shocks
# randomly size the number of shocks
# randomly distribute each of the non stationary shocks

# setting parameter, normalize for the lengths of the series

nstat_bag = function(n){
  # n = number of application of the bag depending on the number of series, input should be a vector of series
  # select number of shocks to fill the bag (max depends on number of increments of series)
  fill_bag = runif(n, min = 0, max = 100)
  # randomly size each item in fill bag
  # assign yes no to each element of the vector
  
}
```






